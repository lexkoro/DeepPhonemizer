
paths:
  checkpoint_dir: checkpoints   # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: datasets            # Directory to store processed data, will be created if not existing.

preprocessing:
  languages: ['de', 'en_us']    # All languages in the dataset.

  # Text (grapheme) and phoneme symbols, either provide a string or list of strings.
  # Symbols in the dataset will be filtered according to these lists!
  text_symbols: 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZäöüÄÖÜß'
  phoneme_symbols: [
    "_",
    "A",
    "å",
    "3",
    "r",
    "9",
    "̃",
    "þ",
    "л",
    "o",
    "ï",
    "г",
    "E",
    "ī",
    "ʰ",
    "ɥ",
    "ʉ",
    "œ",
    "ѣ",
    "ʐ",
    "é",
    "ǔ",
    "ɵ",
    "Z",
    "̩",
    "ʊ",
    "ʙ",
    "n",
    ":",
    "ṛ",
    "V",
    "ˈ",
    "ø",
    "ɨ",
    "ê",
    "ʂ",
    "ъ",
    "я",
    "ʔ",
    "ɦ",
    "ʀ",
    "l",
    "d",
    "̂",
    "ɔ",
    "É",
    "̟",
    "п",
    "õ",
    "O",
    "a",
    "а",
    "g",
    "̯",
    "͡",
    "h",
    "s",
    "!",
    "̺",
    "ы",
    "W",
    "̑",
    "ṭ",
    '"',
    "`",
    "~",
    "á",
    "ʋ",
    "ɟ",
    "Ć",
    "ш",
    "ÿ",
    "ú",
    "f",
    "ю",
    "͜",
    "β",
    "M",
    "ŋ",
    "ѯ",
    "ɹ",
    "ò",
    "Î",
    "ɭ",
    "в",
    "H",
    "I",
    "ʃ",
    "ō",
    "J",
    "∊",
    "̥",
    "ñ",
    "ɝ",
    "Œ",
    "ˑ",
    "Č",
    "₂",
    "м",
    "?",
    "ʈ",
    "ă",
    "Ä",
    "í",
    "ő",
    "х",
    "ˀ",
    "^",
    "ð",
    "ф",
    "о",
    "и",
    "ʒ",
    "D",
    "0",
    "ѵ",
    "θ",
    "̪",
    "ǎ",
    "T",
    "ˇ",
    "щ",
    "ž",
    "w",
    "ɸ",
    "S",
    "ʑ",
    "ù",
    "ö",
    "ı",
    "ɫ",
    "̠",
    "ā",
    "B",
    "F",
    "ɻ",
    "j",
    "y",
    "ц",
    "ł",
    "̝",
    "à",
    "2",
    "ё",
    "т",
    "α",
    "-",
    "ã",
    "ț",
    "ū",
    "ɓ",
    "ý",
    "k",
    "ʻ",
    "Q",
    "ˌ",
    "ç",
    "‿",
    "X",
    "ń",
    "ș",
    "t",
    "ə",
    ";",
    "ɚ",
    "ɾ",
    "ć",
    "э",
    "ѳ",
    "č",
    "ч",
    "Ü",
    "x",
    "к",
    "e",
    "ɬ",
    "ħ",
    "L",
    "й",
    ",",
    "7",
    "š",
    "б",
    "е",
    "？",
    "с",
    "C",
    "û",
    "ř",
    "ʎ",
    "з",
    "ʏ",
    "ɲ",
    "5",
    "u",
    "ɜ",
    "ʁ",
    "̊",
    "̆",
    "ʼ",
    "c",
    "Å",
    "U",
    "у",
    "͂",
    "・",
    "8",
    "ô",
    "â",
    "*",
    "ƶ",
    "ʲ",
    "ɱ",
    "ʳ",
    "+",
    "ğ",
    "ʕ",
    "н",
    "ḷ",
    "̌",
    "ź",
    "ь",
    "д",
    "4",
    "ą",
    "ä",
    "ś",
    "ó",
    "R",
    "v",
    "m",
    "À",
    "z",
    "і",
    "Š",
    "'",
    "ɑ",
    "ɒ",
    "\n",
    "i",
    "ɛ",
    "’",
    "Y",
    "̚",
    "ë",
    "ˠ",
    "ʍ",
    "̞",
    "G",
    "K",
    "q",
    "1",
    "ɪ",
    "N",
    "î",
    "b",
    "μ",
    "ɡ",
    "χ",
    "р",
    "ɣ",
    "P",
    " ",
    "ː",
    "ɘ",
    "ē",
    "ü",
    "ɐ",
    "ę",
    "ż",
    "ạ",
    "p",
    "…",
    "6",
    "æ",
    "̍",
    ".",
    "Ö",
    "ʌ",
    "ɤ",
    "è",
    "ж",
    "ɕ",
]

  char_repeats: 1                # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.
                                 # Set to 1 for autoreg_transformer.
  lowercase: true                # Whether to lowercase the grapheme input.
  n_val: 5000                    # Default number of validation data points if no explicit validation data is provided.


model:
  type: 'autoreg_transformer'        # Whether to use a forward transformer or autoregressive transformer model.
                                     # Choices: ['transformer', 'autoreg_transformer']
  d_model: 512
  d_fft: 2048
  layers: 4
  dropout: 0.2
  heads: 8

training:

  # Hyperparams for learning rate and scheduler.
  # The scheduler is reducing the lr on plateau of phoneme error rate (tested every n_generate_steps).

  learning_rate: 0.0001              # Learning rate of Adam.
  warmup_steps: 10000                # Linear increase of the lr from zero to the given lr within the given number of steps.
  scheduler_plateau_factor: 0.5      # Factor to multiply learning rate on plateau.
  scheduler_plateau_patience: 10     # Number of text generations with no improvement to tolerate.
  batch_size: 32                     # Training batch size.
  batch_size_val: 32                 # Validation batch size.
  epochs: 500                        # Number of epochs to train.
  generate_steps: 10000              # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word
                                     # error rates are calculated for the scheduler.
  validate_steps: 10000              # Interval of training steps to validate the model
                                     # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 100000           # Interval of training steps to save the model.
  n_generate_samples: 10             # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model: true  # Whether to store the raw phoneme dict in the model.
                                     # It will be loaded by the phonemizer object.

