
paths:
  checkpoint_dir: /workspace/pretrained_models/g2p/checkpoints   # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: /workspace/pretrained_models/g2p/datasets             # Directory to store processed data, will be created if not existing.

preprocessing:
  languages: ['cym-sw', 'izh', 'mlt', 'afr', 'ukr', 'ltz', 'nld', 'rus', 'nob', 'eng-uk', 'hun', 'ces', 'eng-us', 'por-bz', 'lit', 'slk', 'spa-la', 'dan', 'ita', 'epo', 'spa-ca', 'ron', 'cat', 'tur', 'ang', 'slv', 'mkd', 'fin', 'deu', 'eus', 'tgl', 'fra', 'por-po', 'cym-nw', 'nci']    # All languages in the dataset.

  # Text (grapheme) and phoneme symbols, either provide a string or list of strings.
  # Symbols in the dataset will be filtered according to these lists!
  text_symbols: "ôð’ḍĺŶъẹąщљỳĊнuóöĞЩжȃґ'īѝіŘqìУŝeбđũvč·ůНĠãÝćVЬхŭȩNøÔŕмтÓUáѯšȘ̈АùaЏяKGŻrñẄỲċĉÌиẅtJhѓдĴýŠÒĽгfgẃśлnРÃğЭòPîűïȅdẀчŰÆßYЃzɛЙÇꝿæвŞꝺRСѕwЛÞșőœʼDёşēȋаЮõÿëШТкcÅОІÙàęłºѳṛďБьâŸĵÉИlВǃEŵ︀èꝾЗÖꞇꞀxåʻÏꞆMkþЯМÍmоĥØѐшOќĆFǂїİŐQꝹПŽȉЌjÊеЫыçoËĤōêҐLTŋ̃íěġĝцAĂÁÎħѵẁЖÄєľрpḌIřȇЕBûṭĦФāsХüėÈäэĈÕŜЦйњЄуŤČăЅÚXțꞁWồSЈÀфÑbјКńЇÛŷHúǁyżĐЧсįūДЉųГı‌ȁѣʕťźéiÂŒЁẂĜŴĎCџзŁňZюžпЊəÜ"
  phoneme_symbols: ["'", 'ô', 'l', 'β', 'ˢ', 't', 'ˤ', 'ˈ', 'h', '˕', '︀', 'ð', 'ᵊ', 'è', 'ʍ', 'ý', 'ǎ', 'ʷ', 'ǔ', '˨', 'χ', 'f', 'ɒ', '̝', 'x', 'ᵝ', 'ɴ', 'ɶ', 'ɵ', 'ɕ', '⁾', 'g', 'ɖ', 'ɨ', '‿', 'ʉ', 'n', 'ʎ', 'ă', 'u', 'ó', 'ö', 'k', 'ɯ', 'ɱ', '˥', 'ʈ', 'ˠ', '̊', 'ò', 'î', 'ɹ', 'ɟ', 'ɤ', 'ʙ', 'ɝ', 'd', 'm', 'ʌ', 'q', 'ì', '̯', '˗', '̆', '˞', '˦', 'θ', 'ɥ', 'ʱ', 'b', 'e', '~', 'z', 'ĩ', 'ɛ', '̬', '́', 'ũ', '̻', 'ʐ', 'ᵈ', '͡', 'ʏ', '᷈', 'æ', 'ˀ', '̙', 'v', '︎', 'ú', 'ĭ', 'y', '̟', '̺', '˔', 'ɑ', 'ʋ', 'ˡ', '̩', '̪', '˩', 'ɪ', 'ʰ', 'ã', 'j', 'w', 'ʊ', 'ɦ', 'ŭ', 'ɔ', 'ɣ', 'ø', 'ç', 'o', '̥', 'œ', 'ʑ', 'ʂ', 'ḭ', 'ʼ', '⁽', '̽', '˧', 'ē', 'ɘ', '̀', 'ê', '̚', 'ʳ', 'ŋ', 'ɭ', 'á', 'õ', 'í', '̃', '↗', 'ě', 'ˣ', 'ɜ', 'ʔ', '̈', '̌', 'ɰ', 'ˌ', '∊', 'ɽ', 'ʕ', 'ʝ', 'é', '̍', 'ħ', 'ɻ', 'i', 'ù', 'a', 'ɐ', 'ⁿ', 'c', '̣', 'ɸ', 'ɳ', 'ɲ', 'à', 'K', 'p', 'ɾ', 'ǐ', 'ẽ', 'ʀ', '̂', 'ɮ', 'ɡ', 'ː', ',', 'ɫ', 'ɚ', 'ǀ', 'r', 'ŏ', 'ɬ', 'â', 'û', '͜', 's', '̠', '̰', 'ǒ', 'ˑ', 'ʃ', 'ʁ', 'ɞ', 'ʲ', 'ä', 'ə', '̞', 'ʒ']

  char_repeats: 1                # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.
                                 # Set to 1 for autoreg_transformer.
  lowercase: false               # Whether to lowercase the grapheme input.
  n_val: 50                    # Default number of validation data points if no explicit validation data is provided.


model:
  type: 'autoreg_transformer'        # Whether to use a forward transformer or autoregressive transformer model.
                                     # Choices: ['transformer', 'autoreg_transformer']
  d_model: 512
  d_fft: 1024
  layers: 4
  dropout: 0.2
  heads: 4

training:

  # Hyperparams for learning rate and scheduler.
  # The scheduler is reducing the lr on plateau of phoneme error rate (tested every n_generate_steps).

  learning_rate: 0.0001              # Learning rate of Adam.
  warmup_steps: 10000                # Linear increase of the lr from zero to the given lr within the given number of steps.
  scheduler_plateau_factor: 0.5      # Factor to multiply learning rate on plateau.
  scheduler_plateau_patience: 10     # Number of text generations with no improvement to tolerate.
  batch_size: 64                     # Training batch size.
  batch_size_val: 32                 # Validation batch size.
  epochs: 500                        # Number of epochs to train.
  generate_steps: 10000              # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word
                                     # error rates are calculated for the scheduler.
  validate_steps: 10000              # Interval of training steps to validate the model
                                     # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 100000           # Interval of training steps to save the model.
  n_generate_samples: 10             # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model: true  # Whether to store the raw phoneme dict in the model.
                                     # It will be loaded by the phonemizer object.
  ddp_backend: 'nccl'                # Backend used by Torch DDP
  ddp_host: 'localhost'              # Hostname used by Torch DDP
  ddp_post: '12355'                    # Port used by Torch DDP
