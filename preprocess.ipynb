{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "# tsv files\n",
    "\n",
    "tsv_files = glob('/home/DeepPhonemizer/tsv/*.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages = [x.split(\"/\")[-1].split(\"_\")[0] for x in tsv_files]\n",
    "set_languages = list(set(all_languages))\n",
    "print(set_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "tsv_files = glob(\"/home/DeepPhonemizer/tsv/*.tsv\")\n",
    "train_data = []\n",
    "all_chars = set()\n",
    "all_phonemes = set()\n",
    "\n",
    "for f in tsv_files:\n",
    "    language_code = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "    with open(\n",
    "        f,\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        lines = f.readlines()\n",
    "    # Prepare data as tuples (lang, word, phoneme)\n",
    "    lines = [l.replace(\" \", \"\").replace(\"\\n\", \"\") for l in lines]\n",
    "    splits = [l.split(\"\\t\") for l in lines]\n",
    "    for grapheme, phoneme in splits:\n",
    "        if len(grapheme) > 0 and len(phoneme) > 0:\n",
    "            all_chars.update(grapheme)\n",
    "            all_phonemes.update(phoneme)\n",
    "            train_data.append((language_code, grapheme, phoneme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 0.01 (max 10) of the data from train_grouped_by_lang per language for validation and remove it from train\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "train_grouped_by_lang = defaultdict(list)\n",
    "validate_grouped_by_lang = defaultdict(list)\n",
    "\n",
    "for lang, word, phoneme in train_data:\n",
    "    train_grouped_by_lang[lang].append((word, phoneme))\n",
    "\n",
    "for lang, data in train_grouped_by_lang.items():\n",
    "    random.shuffle(data)\n",
    "    n = min(1000, int(len(data) * 0.01))\n",
    "    validate_grouped_by_lang[lang].extend(data[:n])\n",
    "    train_grouped_by_lang[lang] = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "validation_data = []\n",
    "for lang, data in train_grouped_by_lang.items():\n",
    "    for grapheme, phoneme in data:\n",
    "        train_data.append((lang, grapheme, phoneme))\n",
    "        \n",
    "for lang, data in validate_grouped_by_lang.items():\n",
    "    for grapheme, phoneme in data:\n",
    "        validation_data.append((lang, grapheme, phoneme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\".join(list(all_chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(all_phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    checkpoint_path = \"/workspace/pretrained_models/g2p/checkpoints/best_model_no_optim.pt\"\n",
    "    phonemizer = Phonemizer.from_checkpoint(checkpoint_path)\n",
    "\n",
    "    sentences = [\n",
    "        \"Aus einem fernen Land, von dem du wahrscheinlich noch nie etwas gehört hast, Marvin.\",\n",
    "        \"Moment, woher kennst du meinen Namen?\",\n",
    "        \"Ich bin eine gute Zuhörerin und hier unter Deck ist es nicht gerade geräumig.\",\n",
    "        \"Du scheinst ein netter Junge zu sein. Ich habe ein Angebot für dich.\",\n",
    "        \"Siehst du den Mistkerl, der die Pfeile schnitzt? Dieses Arschloch hat meine Schatulle genommen und sie bei seiner Beute versteckt. Ich habe gesehen, wie er damit im Frachtraum verschwunden ist.\",\n",
    "        \"Ich dachte, er würde meine Schatulle in einer Truhe verstecken, aber dann konnte ich das laute Knarzen von Schiffsplanken hören. Könntest du dich im Frachtraum mal umsehen und schauen, ob es dort ein Versteck gibt?\",\n",
    "        \"Ich habe deine Schatulle gefunden.\",\n",
    "        \"Das ist ja grossartig! Vielen Dank, mein Junge!\",\n",
    "        \"Im Moment habe ich leider keine Möglichkeit dich für deine Hilfe zu bezahlen. Aber ich verspreche dir, sobald wir auf Archolos sind sorge ich dafür, dass meine Freunde dich entlohnen!\"\n",
    "        \"Sieht aus, als wären wir bald da.\",\n",
    "        \"Die paar zusammengestückelten Minecrawler-Platten haben im underground Tempel einfach zu viele Hits durchgelassen, man!\",\n",
    "        \"Woher willst du das wissen?\",\n",
    "        \"WAS SOLL DAS?\",\n",
    "        \"Das spiel heisst Gothic.\",\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    for text in sentences:\n",
    "        result = phonemizer.phonemise_list([text], lang=\"deu\", expand_acronyms=False)\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"_\" * 100)\n",
    "\n",
    "        for text_word, pred in result.predictions.items():\n",
    "            tokens, probs = pred.phoneme_tokens, pred.token_probs\n",
    "            tokens = ''.join(tokens)\n",
    "            print(f'{text_word} | {tokens} | {pred.confidence}')\n",
    "\n",
    "        print(text)\n",
    "        print(result.phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "duden_lines = []\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv/deu_duden.tsv\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        duden_lines.append(line)\n",
    "\n",
    "\n",
    "duden_words = [line.split(\"\\t\")[0].strip() for line in duden_lines]\n",
    "\n",
    "\n",
    "wikipron_lines = []\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-extra/deu_latn_narrow.tsv\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        wikipron_lines.append(line)\n",
    "\n",
    "wikipron_words = [line.split(\"\\t\")[0].strip() for line in wikipron_lines]\n",
    "\n",
    "print(len(wikipron_words))\n",
    "\n",
    "# filter out words from wikipron that are in duden\n",
    "wikipron_words = set(wikipron_words) - set(duden_words)\n",
    "\n",
    "# now iterate over wikipron_lines and filter out the words that are not in wikipron_words\n",
    "wikipron_lines = [line for line in wikipron_lines if line.split(\"\\t\")[0].strip() in wikipron_words]\n",
    "\n",
    "\n",
    "# write the filtered wikipron_lines to a new file\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-extra/deu_latn_narrow_filtered.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as wf:\n",
    "    for line in wikipron_lines:\n",
    "        wf.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
