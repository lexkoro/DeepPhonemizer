{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "# tsv files\n",
    "\n",
    "tsv_files = glob(\"/home/DeepPhonemizer/tsv-filtered/*.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_languages = [x.split(\"/\")[-1].split(\"_\")[0] for x in tsv_files]\n",
    "set_languages = list(set(all_languages))\n",
    "set_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "tsv_files = glob(\"/home/DeepPhonemizer/tsv-filtered/*.tsv\")\n",
    "train_data = []\n",
    "all_chars = set()\n",
    "all_phonemes = set()\n",
    "\n",
    "for f in tsv_files:\n",
    "    language_code = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "    with open(\n",
    "        f,\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        lines = f.readlines()\n",
    "    # Prepare data as tuples (lang, word, phoneme)\n",
    "    lines = [l.replace(\" \", \"\").replace(\"\\n\", \"\") for l in lines]\n",
    "    splits = [l.split(\"\\t\") for l in lines]\n",
    "    for grapheme, phoneme in splits:\n",
    "        if len(grapheme) > 0 and len(phoneme) > 0:\n",
    "            all_chars.update(grapheme)\n",
    "            all_phonemes.update(phoneme)\n",
    "            train_data.append((language_code, grapheme, phoneme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join(list(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(all_phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 0.01 (max 10) of the data from train_grouped_by_lang per language for validation and remove it from train\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "train_grouped_by_lang = defaultdict(list)\n",
    "validate_grouped_by_lang = defaultdict(list)\n",
    "\n",
    "for lang, word, phoneme in train_data:\n",
    "    train_grouped_by_lang[lang].append((word, phoneme))\n",
    "\n",
    "for lang, data in train_grouped_by_lang.items():\n",
    "    random.shuffle(data)\n",
    "    n = min(1000, int(len(data) * 0.01))\n",
    "    validate_grouped_by_lang[lang].extend(data[:n])\n",
    "    train_grouped_by_lang[lang] = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "validation_data = []\n",
    "for lang, data in train_grouped_by_lang.items():\n",
    "    for grapheme, phoneme in data:\n",
    "        train_data.append((lang, grapheme, phoneme))\n",
    "\n",
    "for lang, data in validate_grouped_by_lang.items():\n",
    "    for grapheme, phoneme in data:\n",
    "        validation_data.append((lang, grapheme, phoneme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_chars = set(\n",
    "    \"ЉhзчьēėЅÇęōòŁГРÈдгЛVґíyДñОЦвşěѯЗрäȅМȉЯwBũšНЭýçЫvȇЮœĺłэуfÅdIаÀľѵźюăồŰʼXąqtШіĘcŹæĎẹŒЃűшЏāșúÚїįÍSgǂðİњѕŚåZИřŻeOṛяKTöÂnнєпĐlǁûѐĄiЩbЄјĆÖʕŘÕmkËѣ’ūХGÆãЧÃќùžzÉīЙŃóıºCљÜжъȃWğѝčбтȩБĽøЕƵYśHsцѳёȁRÄсḌpëĂЈАҐŞDоƶàțЇťЬéŐćNÑÁůѓТЊîżoЌrőЖêxïňк̈JôüõÓиІǃВщхПḍКЁńČȘMFauáďUŽəɛÌŠųџAŕLjâʻУṭфđEØŋФèĞßÿ'меQлPйСÊŤыìȋ126-4α53μÎ8970₂\"\n",
    ")\n",
    "\n",
    "diff = all_chars - old_chars\n",
    "print(\"\".join(list(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_phonemes = set(\n",
    "    [\n",
    "        \"'\",\n",
    "        \"h\",\n",
    "        \"ʷ\",\n",
    "        \"ε\",\n",
    "        \"ɭ\",\n",
    "        \"ʦ\",\n",
    "        \"ē\",\n",
    "        \"⁽\",\n",
    "        \"ʁ\",\n",
    "        \"ò\",\n",
    "        \"ɔ\",\n",
    "        \"‿\",\n",
    "        \"ʝ\",\n",
    "        \"θ\",\n",
    "        \"̻\",\n",
    "        \"ˠ\",\n",
    "        \"ɪ\",\n",
    "        \"y\",\n",
    "        \"̯\",\n",
    "        \"ʙ\",\n",
    "        \"í\",\n",
    "        \"ǎ\",\n",
    "        \"ɘ\",\n",
    "        \"ˌ\",\n",
    "        '\"',\n",
    "        \"ě\",\n",
    "        \"ä\",\n",
    "        \"˗\",\n",
    "        \"ǀ\",\n",
    "        \"w\",\n",
    "        \"ũ\",\n",
    "        \"̪\",\n",
    "        \"ŏ\",\n",
    "        \"ý\",\n",
    "        \"ç\",\n",
    "        \"v\",\n",
    "        \"ɶ\",\n",
    "        \"œ\",\n",
    "        \"͜\",\n",
    "        \"˕\",\n",
    "        \"̥\",\n",
    "        \"ɻ\",\n",
    "        \"f\",\n",
    "        \"d\",\n",
    "        \"ʔ\",\n",
    "        \"ɝ\",\n",
    "        \"ˈ\",\n",
    "        \"ă\",\n",
    "        \"ɥ\",\n",
    "        \"q\",\n",
    "        \"t\",\n",
    "        \"ɬ\",\n",
    "        \"︎\",\n",
    "        \"c\",\n",
    "        \"æ\",\n",
    "        \"∊\",\n",
    "        \"͡\",\n",
    "        \"˦\",\n",
    "        \"ú\",\n",
    "        \"ɚ\",\n",
    "        \"́\",\n",
    "        \"χ\",\n",
    "        \"g\",\n",
    "        \"ɨ\",\n",
    "        \"ð\",\n",
    "        \"ʎ\",\n",
    "        \"ʀ\",\n",
    "        \"ʊ\",\n",
    "        \"˨\",\n",
    "        \"̩\",\n",
    "        \"e\",\n",
    "        \"̌\",\n",
    "        \"n\",\n",
    "        \"̞\",\n",
    "        \"l\",\n",
    "        \"û\",\n",
    "        \"ẽ\",\n",
    "        \"ʋ\",\n",
    "        \"i\",\n",
    "        \"ɹ\",\n",
    "        \"̽\",\n",
    "        \"b\",\n",
    "        \"̝\",\n",
    "        \"ʌ\",\n",
    "        \"ʕ\",\n",
    "        \"ɳ\",\n",
    "        \"ɾ\",\n",
    "        \"ʉ\",\n",
    "        \"β\",\n",
    "        \"m\",\n",
    "        \"ʍ\",\n",
    "        \"k\",\n",
    "        \"ʃ\",\n",
    "        \"ɲ\",\n",
    "        \"ɽ\",\n",
    "        \"ã\",\n",
    "        \"ù\",\n",
    "        \"z\",\n",
    "        \"↗\",\n",
    "        \"ɫ\",\n",
    "        \"ó\",\n",
    "        \"ʳ\",\n",
    "        \",\",\n",
    "        \"ǔ\",\n",
    "        \"ɮ\",\n",
    "        \"ʏ\",\n",
    "        \"ʂ\",\n",
    "        \"̂\",\n",
    "        \"ø\",\n",
    "        \"ɣ\",\n",
    "        \"̟\",\n",
    "        \"s\",\n",
    "        \"ĩ\",\n",
    "        \"ˢ\",\n",
    "        \"ᵻ\",\n",
    "        \"p\",\n",
    "        \"̚\",\n",
    "        \"ɰ\",\n",
    "        \"ʒ\",\n",
    "        \"ᵝ\",\n",
    "        \"à\",\n",
    "        \"é\",\n",
    "        \"ɸ\",\n",
    "        \"ɵ\",\n",
    "        \"^\",\n",
    "        \"î\",\n",
    "        \"ɦ\",\n",
    "        \"o\",\n",
    "        \"r\",\n",
    "        \"ɕ\",\n",
    "        \"ɱ\",\n",
    "        \"ˣ\",\n",
    "        \"˞\",\n",
    "        \"ê\",\n",
    "        \"x\",\n",
    "        \"̈\",\n",
    "        \"ɟ\",\n",
    "        \"ô\",\n",
    "        \"ǐ\",\n",
    "        \"?\",\n",
    "        \"õ\",\n",
    "        \"̠\",\n",
    "        \"ɐ\",\n",
    "        \"ɯ\",\n",
    "        \"̣\",\n",
    "        \"ˀ\",\n",
    "        \"̬\",\n",
    "        \"ʑ\",\n",
    "        \"ʰ\",\n",
    "        \"̃\",\n",
    "        \"⁾\",\n",
    "        \"ɛ\",\n",
    "        \"ɜ\",\n",
    "        \"a\",\n",
    "        \"ː\",\n",
    "        \"u\",\n",
    "        \"ə\",\n",
    "        \"ɑ\",\n",
    "        \"̺\",\n",
    "        \"̀\",\n",
    "        \"̊\",\n",
    "        \"á\",\n",
    "        \"ħ\",\n",
    "        \"j\",\n",
    "        \"â\",\n",
    "        \"ɒ\",\n",
    "        \"̆\",\n",
    "        \"ˑ\",\n",
    "        \"ŋ\",\n",
    "        \"ǒ\",\n",
    "        \"ɤ\",\n",
    "        \"è\",\n",
    "        \"ʲ\",\n",
    "        \"ɡ\",\n",
    "        \"ʐ\",\n",
    "        \"̍\",\n",
    "        \"˧\",\n",
    "        \"˥\",\n",
    "        \"ì\",\n",
    "        \".\",\n",
    "        \"ɓ\",\n",
    "        \"-\",\n",
    "        \"ạ\",\n",
    "        \"̑\",\n",
    "        \"ǃ\",\n",
    "        \"ʈ\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "diff = all_phonemes - old_phonemes\n",
    "print(list(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp.phonemizer import Phonemizer\n",
    "\n",
    "checkpoint_path = (\n",
    "    \"/workspace/pretrained_models/g2p/forward_checkpoints_v3/best_model_no_optim.pt\"\n",
    ")\n",
    "phonemizer = Phonemizer.from_checkpoint(checkpoint_path)\n",
    "\n",
    "sentences = [\n",
    "    \"Aus einem fernen Land, von dem du wahrscheinlich noch nie etwas gehört hast, Marvin.\",\n",
    "    \"Moment, woher kennst du meinen Namen?\",\n",
    "    \"Ich bin eine gute Zuhörerin und hier unter Deck ist es nicht gerade geräumig.\",\n",
    "    \"Du scheinst ein netter Junge zu sein. Ich habe ein Angebot für dich.\",\n",
    "    \"Siehst du den Mistkerl, der die Pfeile schnitzt? Dieses Arschloch hat meine Schatulle genommen und sie bei seiner Beute versteckt. Ich habe gesehen, wie er damit im Frachtraum verschwunden ist.\",\n",
    "    \"Ich dachte, er würde meine Schatulle in einer Truhe verstecken, aber dann konnte ich das laute Knarzen von Schiffsplanken hören. Könntest du dich im Frachtraum mal umsehen und schauen, ob es dort ein Versteck gibt?\",\n",
    "    \"Ich habe deine Schatulle gefunden.\",\n",
    "    \"Das ist ja großartig! Vielen Dank, mein Junge!\",\n",
    "    \"Im Moment habe ich leider keine Möglichkeit dich für deine Hilfe zu bezahlen. Aber ich verspreche dir, sobald wir auf Archolos sind sorge ich dafür, dass meine Freunde dich entlohnen!\"\n",
    "    \"Sieht aus, als wären wir bald da.\",\n",
    "    \"Woher willst du das wissen?\",\n",
    "    \"WAS SOLL DAS?\",\n",
    "    \"Das spiel heißt Gothic.\",\n",
    "    \"Die paar zusammengestückelten Minecrawler-Platten haben im underground Tempel einfach zu viele Hits durchgelassen, man!\",\n",
    "    \"Scavenger Snapper Molerat Vatras\"\n",
    "]\n",
    "\n",
    "\n",
    "for text in sentences:\n",
    "    result = phonemizer.phonemise_list([text], lang=\"deu\", expand_acronyms=True)\n",
    "\n",
    "    print(\"_\" * 100)\n",
    "\n",
    "    for text_word, pred in result.predictions.items():\n",
    "        tokens, probs = pred.phoneme_tokens, pred.token_probs\n",
    "        tokens = \"\".join(tokens)\n",
    "        print(f\"{text_word} | {tokens} | {pred.confidence}\")\n",
    "\n",
    "    print(text)\n",
    "    print(result.phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "duden_lines = []\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-extra/deu_ipa_lexicon.tsv\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        if \"(\" not in line:\n",
    "            duden_lines.append(line)\n",
    "\n",
    "\n",
    "duden_words = [line.split(\"\\t\")[0].strip() for line in duden_lines]\n",
    "\n",
    "\n",
    "wikipron_lines = []\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-filtered/deu_filtered.tsv\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        wikipron_lines.append(line)\n",
    "\n",
    "wikipron_words = [line.split(\"\\t\")[0].strip() for line in wikipron_lines]\n",
    "\n",
    "print(len(wikipron_words))\n",
    "\n",
    "# filter our wikipron words which are also in duden_words\n",
    "wikipron_words = set(wikipron_words) - set(duden_words)\n",
    "\n",
    "# filter out wikipron lines which are not in wikipron_words\n",
    "wikipron_lines = [\n",
    "    line for line in wikipron_lines if line.split(\"\\t\")[0].strip() in wikipron_words\n",
    "]\n",
    "\n",
    "# combine duden and wikipron lines\n",
    "all_lines = duden_lines + wikipron_lines\n",
    "\n",
    "# write the filtered wikipron_lines to a new file\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-filtered/deu_ipa_lexicon.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as wf:\n",
    "    for line in all_lines:\n",
    "        wf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code_mapping = {\n",
    "    \"lit\": \"lt\",\n",
    "    \"spa-ca\": \"es\",\n",
    "    \"eng-uk\": \"en-gb\",\n",
    "    \"hun\": \"hu\",\n",
    "    \"ltz\": \"lb\",\n",
    "    \"ron\": \"ro\",\n",
    "    \"dan\": \"da\",\n",
    "    \"fin\": \"fi\",\n",
    "    \"ces\": \"cs\",\n",
    "    \"fra\": \"fr-fr\",\n",
    "    \"tur\": \"tr\",\n",
    "    \"por-po\": \"pt\",\n",
    "    \"rus\": \"ru\",\n",
    "    \"pol\": \"pl\",\n",
    "    \"nld\": \"nl\",\n",
    "    \"ukr\": \"uk\",\n",
    "    \"slk\": \"sk\",\n",
    "    \"eng-us\": \"en-us\",\n",
    "    \"ita\": \"it\",\n",
    "    \"spa-la\": \"es-419\",\n",
    "    \"slv\": \"sl\",\n",
    "    \"por-bz\": \"pt-br\",\n",
    "    \"deu\": \"de\",\n",
    "    \"mkd\": \"mk\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer.backend import EspeakBackend\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "for lang, lang_code in language_code_mapping.items():\n",
    "    phonemizer = EspeakBackend(\n",
    "        lang_code,\n",
    "        preserve_punctuation=True,\n",
    "        with_stress=False,\n",
    "        tie=False,\n",
    "        language_switch=\"remove-flags\",\n",
    "        words_mismatch=\"ignore\",\n",
    "    )\n",
    "\n",
    "    tsv_files = glob(f\"/home/DeepPhonemizer/tsv/{lang}*.tsv\")\n",
    "\n",
    "    train_data = defaultdict(list)\n",
    "\n",
    "    for f in tsv_files:\n",
    "        language_code = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "        with open(\n",
    "            f,\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "            lines = f.readlines()\n",
    "        # Prepare data as tuples (lang, word, phoneme)\n",
    "        lines = [l.replace(\" \", \"\").replace(\"\\n\", \"\") for l in lines]\n",
    "        splits = [l.split(\"\\t\") for l in lines]\n",
    "        for grapheme, phoneme in splits:\n",
    "            if len(grapheme) > 0 and len(phoneme) > 0:\n",
    "                train_data[grapheme].append(phoneme)\n",
    "\n",
    "    filtered_train_data = []\n",
    "\n",
    "    for key, vals in train_data.items():\n",
    "        if len(vals) > 1:\n",
    "            phonemized = phonemizer.phonemize([key], strip=True, njobs=1)[0]\n",
    "\n",
    "            # calculate the levenshtein distance between the phonemized and the vals\n",
    "            phonemized_to_take = []\n",
    "            for val in vals:\n",
    "                error = levenshtein_distance(phonemized, val)\n",
    "                length = max(len(phonemized), len(val))\n",
    "                error_rate = error / length\n",
    "\n",
    "                # take the phoneme with the lowest error rate\n",
    "                phonemized_to_take.append([val, error_rate])\n",
    "\n",
    "            # take the phoneme with the lowest error rate\n",
    "            phonemized_to_take = sorted(phonemized_to_take, key=lambda x: x[1])\n",
    "\n",
    "            if phonemized_to_take[0][1] < 0.4:\n",
    "                filtered_train_data.append((key, phonemized_to_take[0][0]))\n",
    "            else:\n",
    "                filtered_train_data.append((key, phonemized))\n",
    "\n",
    "        else:\n",
    "            filtered_train_data.append((key, vals[0]))\n",
    "\n",
    "    # write filtered_train_data to new file\n",
    "    with open(\n",
    "        f\"/home/DeepPhonemizer/tsv-filtered/{lang}_filtered.tsv\",\n",
    "        \"w\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as wf:\n",
    "        for key, val in filtered_train_data:\n",
    "            wf.write(f\"{key}\\t{val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer.backend import EspeakBackend\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "phonemizer = EspeakBackend(\n",
    "    \"de\",\n",
    "    preserve_punctuation=True,\n",
    "    with_stress=False,\n",
    "    tie=False,\n",
    "    language_switch=\"remove-flags\",\n",
    "    words_mismatch=\"ignore\",\n",
    ")\n",
    "\n",
    "\n",
    "lang = \"deu\"\n",
    "\n",
    "tsv_files = glob(f\"/home/DeepPhonemizer/tsv/{lang}_ipa*.tsv\")\n",
    "\n",
    "train_data = defaultdict(list)\n",
    "\n",
    "for f in tsv_files:\n",
    "    language_code = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "    with open(\n",
    "        f,\n",
    "        \"r\",\n",
    "        encoding=\"utf-8\",\n",
    "    ) as f:\n",
    "        lines = f.readlines()\n",
    "    # Prepare data as tuples (lang, word, phoneme)\n",
    "    lines = [l.replace(\" \", \"\").replace(\"\\n\", \"\") for l in lines]\n",
    "    splits = [l.split(\"\\t\") for l in lines]\n",
    "    for grapheme, phoneme in splits:\n",
    "        if len(grapheme) > 0 and len(phoneme) > 0:\n",
    "            train_data[grapheme].append(phoneme)\n",
    "\n",
    "filtered_train_data = []\n",
    "\n",
    "for key, vals in train_data.items():\n",
    "    if len(vals) > 1:\n",
    "        phonemized = phonemizer.phonemize([key], strip=True, njobs=1)[0]\n",
    "\n",
    "        # calculate the levenshtein distance between the phonemized and the vals\n",
    "        phonemized_to_take = []\n",
    "        for val in vals:\n",
    "            error = levenshtein_distance(phonemized, val)\n",
    "            length = max(len(phonemized), len(val))\n",
    "            error_rate = error / length\n",
    "\n",
    "            # take the phoneme with the lowest error rate\n",
    "            phonemized_to_take.append([val, error_rate])\n",
    "\n",
    "        # take the phoneme with the lowest error rate\n",
    "        phonemized_to_take = sorted(phonemized_to_take, key=lambda x: x[1])\n",
    "\n",
    "        if phonemized_to_take[0][1] < 0.4:\n",
    "            filtered_train_data.append((key, phonemized_to_take[0][0]))\n",
    "        else:\n",
    "            filtered_train_data.append((key, phonemized))\n",
    "\n",
    "    else:\n",
    "        filtered_train_data.append((key, vals[0]))\n",
    "\n",
    "# write filtered_train_data to new file\n",
    "with open(\n",
    "    f\"/home/DeepPhonemizer/tsv-filtered/{lang}_ipa_filtered.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as wf:\n",
    "    for key, val in filtered_train_data:\n",
    "        wf.write(f\"{key}\\t{val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemized_to_take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "phonemizer = EspeakBackend(\n",
    "    \"en-gb\",\n",
    "    preserve_punctuation=True,\n",
    "    with_stress=False,\n",
    "    tie=False,\n",
    "    language_switch=\"remove-flags\",\n",
    "    words_mismatch=\"ignore\",\n",
    ")\n",
    "\n",
    "\n",
    "# real all words from /home/DeepPhonemizer/tsv-filtered/deu_filtered.tsv\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-filtered/eng-uk_filtered.tsv\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    words = set([line.strip().split(\"\\t\")[0] for line in rf])\n",
    "\n",
    "all_words = []\n",
    "# read all words from /home/DeepPhonemizer/tsv-extra/wordlist-german.txt\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-extra/wordlist-english.txt\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        all_words.append(line.strip())\n",
    "\n",
    "print(len(all_words))\n",
    "# take only the all_german_words that are not in german_words\n",
    "all_words = set(all_words) - words\n",
    "print(len(all_words))\n",
    "\n",
    "# move back to list?\n",
    "all_words = list(all_words)\n",
    "\n",
    "# write new tsv file with all word phoneme pairs\n",
    "with open(\n",
    "    \"/home/DeepPhonemizer/tsv-extra/eng-uk_wordlist.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as wf:\n",
    "    for word in all_words:\n",
    "        phonemized = phonemizer.phonemize([word], strip=True, njobs=1)[0]\n",
    "        wf.write(f\"{word}\\t{phonemized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as lev\n",
    "\n",
    "from phonemizer.backend import EspeakBackend\n",
    "\n",
    "phonemizer = EspeakBackend(\n",
    "    \"de\",\n",
    "    preserve_punctuation=True,\n",
    "    with_stress=False,\n",
    "    tie=False,\n",
    "    language_switch=\"remove-flags\",\n",
    "    words_mismatch=\"ignore\",\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_distance(phoneme1, phoneme2):\n",
    "    return lev.distance(phoneme1, phoneme2)\n",
    "\n",
    "\n",
    "def check_phonemes(grapheme, source_phoneme, reference_phoneme, threshold):\n",
    "    distance = calculate_distance(source_phoneme, reference_phoneme)\n",
    "\n",
    "    length = max(len(reference_phoneme), len(source_phoneme))\n",
    "    error_rate = distance / length\n",
    "\n",
    "    if error_rate > threshold:\n",
    "        print(f\"{grapheme} \\t {source_phoneme} \\t {reference_phoneme} \\t {error_rate}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonemizer.backend import EspeakBackend\n",
    "from glob import glob\n",
    "\n",
    "for lang, lang_code in language_code_mapping.items():\n",
    "    if lang != \"deu\":\n",
    "        continue\n",
    "\n",
    "    phonemizer = EspeakBackend(\n",
    "        lang_code,\n",
    "        preserve_punctuation=True,\n",
    "        with_stress=False,\n",
    "        tie=False,\n",
    "        language_switch=\"remove-flags\",\n",
    "        words_mismatch=\"ignore\",\n",
    "    )\n",
    "\n",
    "    tsv_files = glob(f\"/home/DeepPhonemizer/tsv-filtered/{lang}_ipa*.tsv\")\n",
    "\n",
    "    train_data = []\n",
    "\n",
    "    for f in tsv_files:\n",
    "        language_code = f.split(\"/\")[-1].split(\"_\")[0]\n",
    "        with open(\n",
    "            f,\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "            lines = f.readlines()\n",
    "        # Prepare data as tuples (lang, word, phoneme)\n",
    "        lines = [l.replace(\" \", \"\").replace(\"\\n\", \"\") for l in lines]\n",
    "        splits = [l.split(\"\\t\") for l in lines]\n",
    "        for grapheme, phoneme in splits:\n",
    "            if len(grapheme) > 0 and len(phoneme) > 0:\n",
    "                train_data.append([grapheme, phoneme])\n",
    "\n",
    "    filtered_train_data = []\n",
    "\n",
    "    for grapheme, phoneme in train_data:\n",
    "        phonemized = phonemizer.phonemize([grapheme], strip=True, njobs=1)[0]\n",
    "\n",
    "        if check_phonemes(grapheme, phoneme, phonemized, 0.8):\n",
    "            filtered_train_data.append((grapheme, phoneme))\n",
    "\n",
    "    # # write filtered_train_data to new file\n",
    "    # with open(\n",
    "    #     f\"/home/DeepPhonemizer/tsv-cleaned/{lang}_cleaned.tsv\",\n",
    "    #     \"w\",\n",
    "    #     encoding=\"utf-8\",\n",
    "    # ) as wf:\n",
    "    #     for key, val in filtered_train_data:\n",
    "    #         wf.write(f\"{key}\\t{val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_phoneme = phonemizer.phonemize([\"oh\"], strip=True, njobs=1)[0]\n",
    "\n",
    "print(ref_phoneme)\n",
    "\n",
    "mapping = {\n",
    "    \"oh\": \"olaˈla\",\n",
    "}\n",
    "\n",
    "check_phonemes(mapping, ref_phoneme, 0.6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
